import feedparser
import hashlib
import logging
from datetime import datetime, timezone
from typing import Dict, Any, Iterable, List
from pymongo import MongoClient, ASCENDING, errors
from pymongo.errors import OperationFailure
from ..utils_text import clean_html, normalize_published_date
# --------- Logging setup ---------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --------- RSS feeds ---------
FEEDS: Dict[str, str] = {
    "Newsbeast": "https://www.newsbeast.gr/feed",
    "Naftemporiki": "https://www.naftemporiki.gr/feed/rss",
    "Greek Reporter": "https://greekreporter.com/feed/",
    "The Guardian": "https://www.theguardian.com/world/rss",
    "TechCrunch": "http://feeds.feedburner.com/TechCrunch/",
    "SKAI": "https://www.skai.gr/rss.xml",
    "In.gr": "https://www.in.gr/feed/",
    "ABC News World": "https://abcnews.go.com/abcnews/internationalheadlines",
    "To Vima": "https://www.tovima.gr/feed/",
    "Documento": "https://documentonews.gr/feed/",
    "Eleftheros Typos": "https://eleftherostypos.gr/feed/",
    "TaNea": "https://www.tanea.gr/feed/",
    "Associated Press": "https://feedx.net/rss/ap.xml",
    "NPR News": "https://feeds.npr.org/1001/rss.xml",
    "The Verge": "https://www.theverge.com/rss/index.xml",
    "TechRadar": "https://www.techradar.com/rss",
}

# --------- Î˜ÎµÎ¼Î±Ï„Î¹ÎºÎ­Ï‚ ÎšÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚ (Î±Ï€Î»Î® keyword Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ·) ---------
CATEGORIES: Dict[str, Iterable[str]] = {
    "Politics": ["government", "election", "minister", "policy", "Î²Î¿Ï…Î»Î®", "Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ®"],
    "Economy": ["inflation", "market", "stock", "ÎµÏ€ÎµÎ½Î´ÏÏƒÎµÎ¹Ï‚", "Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¯Î±", "Ï„ÏÎ¬Ï€ÎµÎ¶Î±"],
    "Technology": ["ai", "artificial intelligence", "tech", "blockchain", "software", "Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±"],
    "Health": ["health", "covid", "disease", "hospital", "Ï…Î³ÎµÎ¯Î±", "Î¹ÏŒÏ‚", "ÎµÎ¼Î²ÏŒÎ»Î¹Î¿"],
    "World": ["conflict", "war", "un", "israel", "russia", "world", "Ï€Î±Î³ÎºÏŒÏƒÎ¼Î¹Î¿Ï‚"],
    "Sports": ["match", "team", "goal", "league", "Ï€ÏÏ‰Ï„Î¬Î¸Î»Î·Î¼Î±", "Î¿Î¼Î¬Î´Î±", "Ï€Î¿Î´ÏŒÏƒÏ†Î±Î¹ÏÎ¿"],
}

# --------- MongoDB setup ---------
MONGO_URL = "mongodb://172.25.240.1:27017/"
DB_NAME = "news_database"
COLL_NAME = "articles"

_client = MongoClient(MONGO_URL)
_db = _client[DB_NAME]
collection = _db[COLL_NAME]

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î²Î±ÏƒÎ¹ÎºÏÎ½ index 
try:
    collection.create_index([("link", ASCENDING)], unique=True, name="uniq_link")
    collection.create_index([("hash", ASCENDING)], name="idx_hash")
    collection.create_index([("published_date", ASCENDING)], name="idx_published_date")
except errors.PyMongoError as e:
    logger.warning(f"Index creation warning: {e}")


# --------- Helpers ---------
def classify_category(text: str) -> str:
    t = (text or "").lower()
    for category, keywords in CATEGORIES.items():
        if any(kw in t for kw in keywords):
            return category
    return "General"


def compute_md5(text: str) -> str:
    return hashlib.md5((text or "").encode("utf-8")).hexdigest()


def _extract_raw_html(entry) -> str:
    """Î ÏÎ¿Ï„Î¹Î¼Î¬ entry.content[0].value â†’ entry.summary â†’ ''."""
    try:
        if "content" in entry and entry.content:
            return entry.content[0].value or ""
        if "summary" in entry:
            return entry.summary or ""
    except Exception:
        pass
    return ""


def _pick_entry_date_iso(entry) -> str:
    """
    Î ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î±: published_parsed â†’ updated_parsed â†’ published â†’ updated â†’ now.
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î Î‘ÎÎ¤Î‘ ISO ÏƒÎµ UTC.
    """
    for key in ("published_parsed", "updated_parsed", "published", "updated"):
        if key in entry and entry[key]:
            return normalize_published_date(entry[key])
    return normalize_published_date(datetime.now(timezone.utc))


# --------- Core crawling ---------
def crawl(limit_per_feed: int = 15, min_content_len: int = 400) -> None:
    """
    Î”Î¹Î±Ï„ÏÎ­Ï‡ÎµÎ¹ ÏŒÎ»Î± Ï„Î± FEEDS, ÎºÎ±Î¸Î±ÏÎ¯Î¶ÎµÎ¹ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿/Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚ ÎºÎ±Î¹
    ÎµÎ¹ÏƒÎ¬Î³ÎµÎ¹/ÎµÎ½Î·Î¼ÎµÏÏÎ½ÎµÎ¹ Î¬ÏÎ¸ÏÎ± ÏƒÏ„Î· Mongo Î¼Îµ dedup (link/hash).
    """
    logger.info(" ÎÎµÎºÎ¯Î½Î·ÏƒÎµ Ï„Î¿ crawling ÎµÎ¹Î´Î®ÏƒÎµÏ‰Î½...")
    new_articles: List[Dict[str, Any]] = []
    inserted = updated = skipped = 0

    for source_name, feed_url in FEEDS.items():
        logger.info(f"ğŸ“¡ Crawling {source_name} ...")
        try:
            feed = feedparser.parse(feed_url)
            entries = getattr(feed, "entries", []) or []
            if not entries:
                logger.warning(f"âš ï¸ ÎšÎµÎ½ÏŒ feed: {source_name}")
                continue

            for entry in entries[:limit_per_feed]:
                # Î ÎµÎ´Î¯Î±
                title_raw = (entry.get("title") or "").strip()
                link = (entry.get("link") or "").strip()
                raw_html = _extract_raw_html(entry)

                # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚
                title = clean_html(title_raw)
                content = clean_html(raw_html)
                published_date = _pick_entry_date_iso(entry)

                # Î Î¿Î¹ÏŒÏ„Î·Ï„Î±
                if not title or not link or len(content) < min_content_len:
                    skipped += 1
                    continue

                category = classify_category(f"{title} {content} {link}")
                content_hash = compute_md5(content)

                doc = {
                    "title": title,
                    "link": link,
                    "published_date": published_date,  # ISO UTC
                    "fetched_at": datetime.now(timezone.utc).isoformat(),
                    "content": content,
                    "category": category,
                    "source": source_name,
                    "hash": content_hash,
                }

                # Insert / Update (Î¼Îµ Î²Î¬ÏƒÎ· link Î® Î´Î¹Î±Ï†Î¿ÏÎ¿Ï€Î¿Î¯Î·ÏƒÎ· hash)
                try:
                    existing = collection.find_one(
                        {"$or": [{"link": doc["link"]}, {"hash": doc["hash"]}]},
                        projection={"_id": 1, "hash": 1},
                    )
                    if not existing:
                        collection.insert_one(doc)
                        inserted += 1
                        new_articles.append(doc)
                    elif existing.get("hash") != doc["hash"]:
                        collection.update_one({"_id": existing["_id"]}, {"$set": doc})
                        updated += 1
                    else:
                        skipped += 1
                except errors.PyMongoError as e:
                    logger.error(f"âŒ Mongo error ({source_name}): {e}")
                    skipped += 1

            logger.info(f"âœ… {source_name}: inserted={inserted}, updated={updated}, skipped={skipped}")

        except Exception as e:
            logger.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î¿ crawling Ï„Î¿Ï… {source_name}: {e}")

    total = inserted + updated
    if total:
        logger.info(f"ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ Ï„Î¿ crawling! ÎÎ­Î±/ÎµÎ½Î·Î¼ÎµÏÏ‰Î¼Î­Î½Î± Î¬ÏÎ¸ÏÎ±: {total} (inserted={inserted}, updated={updated}).")
    else:
        logger.warning("âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î½Î­Î±/ÎµÎ½Î·Î¼ÎµÏÏ‰Î¼Î­Î½Î± Î¬ÏÎ¸ÏÎ±.")


# --------- Optional CLI entrypoint ---------
if __name__ == "__main__":
    crawl()