# app/crawler/crawler.py

import feedparser
import hashlib
from pymongo import MongoClient
from datetime import datetime
from apscheduler.schedulers.background import BackgroundScheduler

# --------- RSS feeds ---------
FEEDS = {
    "Newsbeast": "https://www.newsbeast.gr/feed",
    "Naftemporiki": "https://www.naftemporiki.gr/feed/rss",
    "Greek Reporter": "https://greekreporter.com/feed/",
    "The Guardian": "https://www.theguardian.com/world/rss",
    "CNN World": "http://rss.cnn.com/rss/edition_world.rss",
    "TechCrunch": "http://feeds.feedburner.com/TechCrunch/"
}

# --------- Î˜ÎµÎ¼Î±Ï„Î¹ÎºÎ­Ï‚ ÎšÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚ ---------
CATEGORIES = {
    "Politics": ["government", "election", "minister", "policy", "Î²Î¿Ï…Î»Î®", "Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ®"],
    "Economy": ["inflation", "market", "stock", "ÎµÏ€ÎµÎ½Î´ÏÏƒÎµÎ¹Ï‚", "Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¯Î±", "Ï„ÏÎ¬Ï€ÎµÎ¶Î±"],
    "Technology": ["ai", "artificial intelligence", "tech", "blockchain", "software", "Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±"],
    "Health": ["health", "covid", "disease", "hospital", "Ï…Î³ÎµÎ¯Î±", "Î¹ÏŒÏ‚", "ÎµÎ¼Î²ÏŒÎ»Î¹Î¿"],
    "World": ["conflict", "war", "UN", "Israel", "Russia", "world", "Ï€Î±Î³ÎºÏŒÏƒÎ¼Î¹Î¿Ï‚"],
    "Sports": ["match", "team", "goal", "league", "Ï€ÏÏ‰Ï„Î¬Î¸Î»Î·Î¼Î±", "Î¿Î¼Î¬Î´Î±", "Ï€Î¿Î´ÏŒÏƒÏ†Î±Î¹ÏÎ¿"],
}

# --------- MongoDB setup ---------
client = MongoClient("mongodb://172.25.240.1:27017/")
db = client["news_database"]
collection = db["articles"]

def classify_category(text):
    text = text.lower()
    for category, keywords in CATEGORIES.items():
        if any(keyword.lower() in text for keyword in keywords):
            return category
    return "General"

def compute_md5(text):
    return hashlib.md5(text.encode("utf-8")).hexdigest()

def crawl():
    print(" ÎÎµÎºÎ¯Î½Î·ÏƒÎµ Ï„Î¿ crawling ÎµÎ¹Î´Î®ÏƒÎµÏ‰Î½...")
    articles = []

    for name, feed_url in FEEDS.items():
        print(f" Crawling {name}...")
        try:
            feed = feedparser.parse(feed_url)
            fetched_articles = []

            for entry in feed.entries[:15]:
                title = entry.get("title", "").strip()
                link = entry.get("link", "").strip()
                published_date = entry.get("published", datetime.now().isoformat())
                content = entry.get("summary", "") or entry.get("description", "")
                content = content.strip()

                # Î Î¿Î¹ÏŒÏ„Î·Ï„Î± Î¬ÏÎ¸ÏÎ¿Ï…
                if not title or not link or len(content) < 300:
                    continue

                category = classify_category(title + " " + content)
                content_hash = compute_md5(content)

                fetched_articles.append({
                    "title": title,
                    "link": link,
                    "published_date": published_date,
                    "content": content,
                    "category": category,
                    "hash": content_hash
                })

            articles.extend(fetched_articles)
            print(f" Crawled {len(fetched_articles)} articles from {name}\n")

        except Exception as e:
            print(f" Error crawling {name}: {e}\n")

    if articles:
        insert_articles_to_mongo(articles)
        print(f" ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ Ï„Î¿ crawling! Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Î¬ÏÎ¸ÏÎ± Î¼Î±Î¶ÎµÏÏ„Î·ÎºÎ±Î½: {len(articles)}\n")
    else:
        print(" Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î¬ÏÎ¸ÏÎ±!\n")

def insert_articles_to_mongo(articles):
    for article in articles:
        existing = collection.find_one({"link": article["link"]})
        if not existing:
            collection.insert_one(article)
        elif existing.get("hash") != article["hash"]:
            collection.update_one({"_id": existing["_id"]}, {"$set": article})
            print(f"ğŸ” Î•Î½Î·Î¼ÎµÏÏÎ¸Î·ÎºÎµ Î¬ÏÎ¸ÏÎ¿: {article['link']}")
        else:
            print(f"âš ï¸ Î‰Î´Î· Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Ï‡Ï‰ÏÎ¯Ï‚ Î±Î»Î»Î±Î³Î®: {article['link']}")

def scheduled_crawl():
    crawl()
    scheduler = BackgroundScheduler()
    scheduler.add_job(crawl, "interval", hours=1)
    scheduler.start()
    print(" Î ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¯ÏƒÏ„Î·ÎºÎµ crawling ÎºÎ¬Î¸Îµ 1 ÏÏÎ±!\n")
