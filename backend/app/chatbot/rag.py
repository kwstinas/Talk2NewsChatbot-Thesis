# backend/app/chatbot/rag.py
from datetime import datetime, timedelta, timezone
from dateutil import parser
import logging
import math
import re
import re as _re
import os
import re as _re_tok
from time import perf_counter
from dotenv import load_dotenv
from .vectorstore import load_vectorstore, similarity_search  # similarity_search ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ [(doc, raw_score), ...]
from .llm import load_llm, generate_answer
from rank_bm25 import BM25Okapi

# --- ÎÎ•Î‘ imports Î³Î¹Î± extractive snippets ---
import regex as re2
import numpy as np
from langchain_huggingface import HuggingFaceEmbeddings
# -------------------------------------------

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· .env (ÏÏƒÏ„Îµ Ï„Î± flags Î½Î± Î´Î¹Î±Î²Î¬Î¶Î¿Î½Ï„Î±Î¹ ÏƒÏ‰ÏƒÏ„Î¬)
load_dotenv()

# Feature flags & ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚
RERANK_BM25_ENABLED = os.getenv("RERANK_BM25_ENABLED", "true").lower() == "true"
EXTRACTIVE_ENABLED  = os.getenv("EXTRACTIVE_ENABLED", "true").lower() == "true"
SNIPPETS_K          = int(os.getenv("SNIPPETS_K", "5"))
SNIPPETS_MIN_SIM    = float(os.getenv("SNIPPETS_MIN_SIM", "0.15"))

# Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ (Î²Î±ÏÏÏ„ÎµÏÎ¿): cross-encoder re-rank (Î´ÎµÎ½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ ÎµÎ´Ï Î±ÎºÏŒÎ¼Î·)
CROSS_ENCODER_ENABLED = os.getenv("CROSS_ENCODER_ENABLED", "false").lower() == "true"
CROSS_ENCODER_MODEL   = os.getenv("CROSS_ENCODER_MODEL", "cross-encoder/ms-marco-MiniLM-L-6-v2")

# Ï€Î¿Î»Ï Î±Ï€Î»Î® tokenization Î³Î¹Î± en/el
_punct_re = _re_tok.compile(r"[^\wÎ†-Ï]+", _re_tok.UNICODE)

def _tok(text: str):
    t = (text or "").lower()
    t = _punct_re.sub(" ", t)
    return [w for w in t.split() if len(w) > 1]

def _bm25_rerank(hits_with_raw, query: str, top_m: int = 10):
    """
    hits_with_raw: [(doc, raw_score), ...] Î±Ï€ÏŒ FAISS
    ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î™Î”Î™Î‘ Î´Î¿Î¼Î® Î±Î»Î»Î¬ re-ordered Î±Ï€ÏŒ BM25 Ï€Î¬Î½Ï‰ ÏƒÎµ (title+content).
    """
    if not hits_with_raw:
        return hits_with_raw
    docs = [d for d, _ in hits_with_raw]
    corpus = [_tok(f"{d.metadata.get('title','')} {d.page_content or ''}") for d in docs]
    bm25 = BM25Okapi(corpus)
    qtok = _tok(query)
    scores = bm25.get_scores(qtok)
    ranked = sorted(zip(hits_with_raw, scores), key=lambda x: x[1], reverse=True)
    reord = [pair for (pair, _score) in ranked[:top_m]]
    return reord

# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· ÎºÎ±Ï„Î±Î³ÏÎ±Ï†Î®Ï‚ ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½
logging.basicConfig(level=logging.INFO)

# ğŸ”¹ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· vectorstore ÏƒÏ„Î¿ startup (Î¯Î´Î¹Î¿ ÏŒÏ€Ï‰Ï‚ Ï€ÏÎ¹Î½)
vectorstore = load_vectorstore()

# ğŸ”¹ Lazy-loading Ï„Î¿Ï… LLM (Î¯Î´Î¹Î¿ ÏŒÏ€Ï‰Ï‚ Ï€ÏÎ¹Î½)
llm_instance = None
def get_llm():
    global llm_instance
    if llm_instance is None:
        llm_instance = load_llm()
    return llm_instance


# =========================
# Topic helpers
# =========================
TOPIC_KEYWORDS = {
    "ai": ["ai", "artificial intelligence", "machine learning", "neural", "open-source ai", "Ï„ÎµÏ‡Î½Î·Ï„Î® Î½Î¿Î·Î¼Î¿ÏƒÏÎ½Î·"],
    "politics": ["politics", "political", "policy", "minister", "government", "Î²Î¿Ï…Î»Î®", "Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ®", "ÎºÏŒÎ¼Î¼Î±"],
    "sports": ["sports", "ÏƒÏ€Î¿Ï", "athletics", "match", "game", "team", "league", "Ï€ÏÏ‰Ï„Î¬Î¸Î»Î·Î¼Î±"],
    "football": ["football", "soccer", "Ï€Î¿Î´ÏŒÏƒÏ†", "goal", "match", "league", "premier league", "uefa", "champions"],
    "greece": ["greece", "greek", "athens", "ÎµÎ»Î»Î¬Î´Î±", "ÎµÎ»Î»Î¬Î´Î±", "Î±Î¸Î®Î½Î±", "ÎºÏ…Î²Î­ÏÎ½Î·ÏƒÎ·", "ÎµÎ»Î»Î·Î½"],
}

def _keywords_for_query(q: str):
    ql = (q or "").lower()
    found = []
    for _topic, kws in TOPIC_KEYWORDS.items():
        if any(k in ql for k in kws):
            found.extend(kws)
    return list(set(found))

def _text_contains_any(text: str, kws) -> bool:
    if not kws:
        return True
    tl = (text or "").lower()
    return any(k in tl for k in kws)


# (Î”Î™Î‘Î¤Î—Î¡Î•Î™Î¤Î‘Î™ Î³Î¹Î± Î¼ÎµÎ»Î»Î¿Î½Ï„Î¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·)
def filter_recent_documents(documents, days=30, desired_category=None):
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
    filtered_docs = []
    for doc in documents:
        metadata = doc.metadata
        published_date_str = metadata.get("published_date")
        category = metadata.get("category", "General").lower()
        if not published_date_str:
            continue
        try:
            published_date = parser.parse(published_date_str)
            if published_date.tzinfo is None:
                published_date = published_date.replace(tzinfo=timezone.utc)
            if published_date >= cutoff_date:
                if desired_category:
                    if category == desired_category.lower():
                        filtered_docs.append((doc, published_date))
                else:
                    filtered_docs.append((doc, published_date))
        except Exception as e:
            logging.error(f"âš ï¸ Î£Ï†Î¬Î»Î¼Î± parsing Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±Ï‚: {published_date_str} -> {e}")
            continue
    logging.info(f"ğŸ§ª ÎÎ­Î± Ï†Î¯Î»Ï„ÏÎ± Î¬ÏÎ¸ÏÏ‰Î½: {len(filtered_docs)} / {len(documents)}")
    return filtered_docs


# =========================
# Scoring params & helpers
# =========================
MAX_AGE_DAYS = 45
RECENCY_WEIGHT = 0.25   # 25% ÎµÏ€Î¯Î´ÏÎ±ÏƒÎ· Ï†ÏÎµÏƒÎºÎ¬Î´Î±Ï‚
TAU_DAYS = 15.0         # ÎµÎºÎ¸ÎµÏ„Î¹ÎºÎ® Î±Ï€Î¿ÏƒÏÎ½Î¸ÎµÏƒÎ· (ÏŒÏƒÎ¿ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ¿, Ï„ÏŒÏƒÎ¿ Â«Ï„Î¹Î¼Ï‰ÏÎµÎ¯Â» Ï„Î± Ï€Î±Î»Î¹Î¬)
MIN_SIM = 0.20          # ÎºÎ±Ï„ÏÏ†Î»Î¹ Î¿Î¼Î¿Î¹ÏŒÏ„Î·Ï„Î±Ï‚ (Î¼ÎµÏ„Î¬ Ï„Î¿Î½ Î¼ÎµÏ„Î±ÏƒÏ‡Î·Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒ)
MIN_LEN = 300           # ÎµÎ»Î¬Ï‡Î¹ÏƒÏ„Î¿ ÎºÎ±Î¸Î±ÏÏŒ Î¼Î®ÎºÎ¿Ï‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…

def _to_utc(dt_str):
    """ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Î¿Ï€Î¿Î¹Î¿Î´Î®Ï€Î¿Ï„Îµ date string ÏƒÎµ aware UTC datetime."""
    if not dt_str:
        return None
    dt = parser.parse(dt_str)
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc)

def _age_days(dt_str):
    dt = _to_utc(dt_str)
    if not dt:
        return 10**9
    return max(0.0, (datetime.now(timezone.utc) - dt).total_seconds() / 86400.0)

def _recency_boost(dt_str):
    """0..1 boost: 1 Î³Î¹Î± Ï€Î¿Î»Ï Ï€ÏÏŒÏƒÏ†Î±Ï„Î¿, ~0 Î³Î¹Î± Ï€Î±Î»Î¹ÏŒ. ÎšÏŒÎ²Î¿Ï…Î¼Îµ Î¬ÏÎ¸ÏÎ± >MAX_AGE_DAYS."""
    age = _age_days(dt_str)
    if age > MAX_AGE_DAYS:
        return 0.0
    return math.exp(-age / TAU_DAYS)

def _distance_to_similarity(raw_score: float) -> float:
    """
    ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® FAISS distance -> pseudo-similarity âˆˆ [0,1].
    """
    try:
        d = float(raw_score)
    except Exception:
        d = 1.0
    return 1.0 / (1.0 + d) if d > 1.0 else max(0.0, 1.0 - d)

def _final_score(similarity: float, published_iso: str) -> float:
    r = _recency_boost(published_iso)
    return similarity * (1.0 - RECENCY_WEIGHT) + RECENCY_WEIGHT * r


def _select_single_article(hits_with_raw, query_kws=None):
    """
    hits_with_raw: Î»Î¯ÏƒÏ„Î± Î±Ï€ÏŒ tuples (doc, raw_score)
    -> ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ (doc, topic_match: bool) Î® (None, False)
    """
    query_kws = query_kws or []
    candidates = []
    for doc, raw in hits_with_raw:
        sim = _distance_to_similarity(raw)
        if sim < MIN_SIM:
            continue
        text = (doc.page_content or "").strip()
        if len(text) < MIN_LEN:
            continue
        pub = doc.metadata.get("published_date")
        if _recency_boost(pub) == 0.0:
            continue

        # topic match: Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ Î­Î½Î± keyword Î¼Î­ÏƒÎ± ÏƒÏ„Î¿ title+content
        topic_match = _text_contains_any(f"{doc.metadata.get('title','')} {text}", query_kws)

        score = _final_score(sim, pub) + (0.03 if topic_match and query_kws else 0.0)  # Î¼Î¹ÎºÏÏŒ bonus Î±Î½ Ï„Î±Î¹ÏÎ¹Î¬Î¶ÎµÎ¹
        candidates.append((score, doc, sim, pub, topic_match))

    if not candidates:
        return None, False

    candidates.sort(key=lambda x: x[0], reverse=True)
    best_score, best_doc, best_sim, best_pub, best_match = candidates[0]
    logging.info(
        f"ğŸ” Î•Ï€Î¹Î»Î­Ï‡Î¸Î·ÎºÎµ Î¬ÏÎ¸ÏÎ¿: '{best_doc.metadata.get('title','â€”')}' "
        f"| sim={best_sim:.3f}, rec={_recency_boost(best_pub):.3f}, final={best_score:.3f}, "
        f"date={best_pub}, topic_match={best_match}"
    )
    return best_doc, best_match


# =========================
# Extractive snippets helpers (ÎÎ•Î‘)
# =========================
_embedder = None
def _get_embedder():
    global _embedder
    if _embedder is None:
        _embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return _embedder

# Î±Ï€Î»ÏŒÏ‚ splitter Ï€Î¿Ï… Î´Î¿Ï…Î»ÎµÏÎµÎ¹ en/el (., ?, !, ÎµÎ»Î»Î·Î½Î¹ÎºÎ® Î¬Î½Ï‰ Ï„ÎµÎ»ÎµÎ¯Î± 'Â·', ellipsis â€¦)
_SENT_SPLIT_RE = re2.compile(r'(?<=[\.\?\!Î‡â€¦])\s+(?=[A-ZÎ‘-Î©Î‰ÎŠÎŒÎÎÎ†ÎˆÎ‰ÎŠÎŒÎÎ])')

def _split_sentences(text: str):
    txt = (text or "").strip()
    parts = _SENT_SPLIT_RE.split(txt) if txt else []
    # Ï€Î­Ï„Î± Ï€Î¿Î»Ï ÎºÎ¿Î½Ï„Î­Ï‚/ÏƒÎºÎ¿Ï…Ï€Î¹Î´Î­Î½Î¹ÎµÏ‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ ÎºÏŒÏˆÎµ Ï…Ï€ÎµÏÎ²Î¿Î»Î¹ÎºÏŒ Î¼Î®ÎºÎ¿Ï‚
    return [s.strip() for s in parts if len(s.strip()) >= 40][:60]

def _cos(a, b):
    denom = (np.linalg.norm(a) * np.linalg.norm(b)) or 1e-9
    return float(np.dot(a, b) / denom)

def _mmr_select(sentences, query_vec, sent_vecs, k=4, lambda_param=0.7):
    if not sentences:
        return [], []
    sims = np.array([_cos(v, query_vec) for v in sent_vecs])
    selected = []
    candidates = list(range(len(sentences)))
    # Ï€ÏÏÏ„Î¿: max relevance
    first = int(np.argmax(sims))
    selected.append(first)
    candidates.remove(first)
    while len(selected) < min(k, len(sentences)):
        mmr_scores = []
        for i in candidates:
            rel = sims[i]
            div = max(_cos(sent_vecs[i], sent_vecs[j]) for j in selected)
            mmr = lambda_param * rel - (1.0 - lambda_param) * div
            mmr_scores.append((mmr, i))
        mmr_scores.sort(reverse=True)
        best_i = mmr_scores[0][1]
        selected.append(best_i)
        candidates.remove(best_i)
    return [sentences[i] for i in selected], [sims[i] for i in selected]

def _extractive_snippets(doc, user_query, k=4, min_sim=0.20):
    text = (doc.page_content or "").strip()
    sents = _split_sentences(text)
    if not sents:
        return []
    emb = _get_embedder()
    try:
        sent_vecs = np.array(emb.embed_documents(sents))
        query_vec = np.array(emb.embed_query(user_query))
    except Exception as _e:
        logging.warning(f"Embed error (snippets): {_e}")
        return []
    chosen, sims = _mmr_select(sents, query_vec, sent_vecs, k=k, lambda_param=0.7)
    if chosen and float(np.mean(sims)) >= min_sim:
        return chosen
    return []


def _build_prompt_one_article(user_query: str, doc, lang: str, topic_match: bool) -> str:
    title = doc.metadata.get("title", "â€”")
    link = doc.metadata.get("link", "")
    pub_iso_dt = _to_utc(doc.metadata.get("published_date"))
    pub_iso = pub_iso_dt.isoformat() if pub_iso_dt else "Î†Î³Î½Ï‰ÏƒÏ„Î· Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±"
    content = (doc.page_content or "").strip()[:1200]

    if lang == "en":
        return f"""
[SYSTEM]
You are a news assistant. Answer STRICTLY from the SINGLE article text below.

OUTPUT RULES (MUST FOLLOW EXACTLY):
- Write a single paragraph with 3â€“5 sentences. No headings, no lists, no labels.
- First sentence MUST begin with: "Based on the {pub_iso} article:"
- Use ONLY the article. Do NOT invent facts. Do NOT ask the user questions.
- Mention the UTC date only in the first sentence. Include the source link only ONCE as the LAST token: {link}
- Do NOT mention internal flags or sections. Do NOT write the words "Topic match", "Title", "Source", "Excerpt", or similar.
- If and only if the article does NOT cover the user's topic directly, add one short note as the LAST sentence before the link: "Note: the provided article does not cover the requested topic directly."
- Do NOT produce lists or bullet points under any circumstance.

[ARTICLE TEXT]
Title: {title}
Date (UTC): {pub_iso}
Link: {link}
Excerpt:
\"\"\"
{content}
\"\"\"

[USER QUESTION]
{user_query}

[INTERNAL FLAG â€” DO NOT MENTION IN THE ANSWER]
topic_match={str(topic_match).lower()}
""".strip()

    # default: Greek
    return f"""
[Î£Î¥Î£Î¤Î—ÎœÎ‘]
Î•Î¯ÏƒÎ±Î¹ Î²Î¿Î·Î¸ÏŒÏ‚ ÎµÎ¹Î´Î®ÏƒÎµÏ‰Î½. Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ Î‘Î ÎŸÎšÎ›Î•Î™Î£Î¤Î™ÎšÎ‘ Î±Ï€ÏŒ Ï„Î¿ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Î•ÎÎ‘ Î¬ÏÎ¸ÏÎ¿.

ÎšÎ‘ÎÎŸÎÎ•Î£ Î•ÎÎŸÎ”ÎŸÎ¥ (Î‘ÎšÎ¡Î™Î’Î©Î£ Î•Î¤Î£Î™):
- Î“ÏÎ¬ÏˆÎµ Î­Î½Î± ÎµÎ½Î¹Î±Î¯Î¿ Ï€Î±ÏÎ¬Î³ÏÎ±Ï†Î¿ Î¼Îµ 3â€“5 Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚. Î§Ï‰ÏÎ¯Ï‚ ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´ÎµÏ‚, Ï‡Ï‰ÏÎ¯Ï‚ Î»Î¯ÏƒÏ„ÎµÏ‚, Ï‡Ï‰ÏÎ¯Ï‚ labels.
- Î— Ï€ÏÏÏ„Î· Ï€ÏÏŒÏ„Î±ÏƒÎ· Î Î¡Î•Î Î•Î™ Î½Î± Î¾ÎµÎºÎ¹Î½Î¬ Î¼Îµ: "Î’Î¬ÏƒÎµÎ¹ Î¬ÏÎ¸ÏÎ¿Ï… Ï„Î·Ï‚ {pub_iso}:"
- Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ ÎœÎŸÎÎŸ Ï„Î¿ Î¬ÏÎ¸ÏÎ¿. ÎœÎ—Î ÎµÏ†ÎµÏ…ÏÎ¯ÏƒÎºÎµÎ¹Ï‚ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î±. ÎœÎ—Î ÏÏ‰Ï„Î¬Ï‚ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·.
- Î‘Î½Î¬Ï†ÎµÏÎµ Ï„Î·Î½ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± UTC Î¼ÏŒÎ½Î¿ ÏƒÏ„Î·Î½ Ï€ÏÏÏ„Î· Ï€ÏÏŒÏ„Î±ÏƒÎ·. Î’Î¬Î»Îµ Ï„Î¿ link ÎœÎŸÎÎŸ ÎœÎ™Î‘ Ï†Î¿ÏÎ¬ Ï‰Ï‚ Î¤Î•Î›Î•Î¥Î¤Î‘Î™ÎŸ token: {link}
- ÎœÎ—Î Î±Î½Î±Ï†Î­ÏÎµÎ¹Ï‚ ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ­Ï‚ ÏƒÎ·Î¼Î±Î¯ÎµÏ‚/sections. ÎœÎ—Î Î³ÏÎ¬ÏˆÎµÎ¹Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ ÏŒÏ€Ï‰Ï‚ "Î£Î·Î¼Î±Î¯Î±", "Topic match", "Î¤Î¯Ï„Î»Î¿Ï‚", "Î Î·Î³Î®", "Î‘Ï€ÏŒÏƒÏ€Î±ÏƒÎ¼Î±".
- ÎœÎŸÎÎŸ Î±Î½ Ï„Î¿ Î¬ÏÎ¸ÏÎ¿ Î´ÎµÎ½ Ï„Î±Î¹ÏÎ¹Î¬Î¶ÎµÎ¹ Î¬Î¼ÎµÏƒÎ± Ï„Î¿ Î¶Î·Ï„Î¿ÏÎ¼ÎµÎ½Î¿ Î¸Î­Î¼Î±, Ï€ÏÏŒÏƒÎ¸ÎµÏƒÎµ ÎœÎ™Î‘ ÏƒÏÎ½Ï„Î¿Î¼Î· Ï„ÎµÎ»Î¹ÎºÎ® Ï€ÏÏŒÏ„Î±ÏƒÎ· Ï€ÏÎ¹Î½ Ï„Î¿ link: "Î£Î·Î¼ÎµÎ¯Ï‰ÏƒÎ·: Ï„Î¿ Î¬ÏÎ¸ÏÎ¿ Î´ÎµÎ½ ÎºÎ±Î»ÏÏ€Ï„ÎµÎ¹ Î¬Î¼ÎµÏƒÎ± Ï„Î¿ Î¶Î·Ï„Î¿ÏÎ¼ÎµÎ½Î¿ Î¸Î­Î¼Î±."
- ÎœÎ—Î Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï‚ Î»Î¯ÏƒÏ„ÎµÏ‚ Î® bullets Î³Î¹Î± ÎºÎ±Î½Î­Î½Î±Î½ Î»ÏŒÎ³Î¿.

[ÎšÎ•Î™ÎœÎ•ÎÎŸ Î‘Î¡Î˜Î¡ÎŸÎ¥]
Î¤Î¯Ï„Î»Î¿Ï‚: {title}
Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± (UTC): {pub_iso}
Î£ÏÎ½Î´ÎµÏƒÎ¼Î¿Ï‚: {link}
Î‘Ï€ÏŒÏƒÏ€Î±ÏƒÎ¼Î±:
\"\"\"
{content}
\"\"\"

[Î•Î¡Î©Î¤Î—Î£Î— Î§Î¡Î—Î£Î¤Î—]
{user_query}

[Î•Î£Î©Î¤Î•Î¡Î™ÎšÎ— Î£Î—ÎœÎ‘Î™Î‘ â€” ÎœÎ—Î Î¤Î—Î Î‘ÎÎ‘Î¦Î•Î¡Î•Î™Î£ Î£Î¤Î—Î Î‘Î Î‘ÎÎ¤Î—Î£Î—]
topic_match={str(topic_match).lower()}
""".strip()


# --- ÎÎ•ÎŸ prompt Î³Î¹Î± extractive snippets ---
def _build_prompt_from_snippets(user_query: str, doc, snippets: list[str], lang: str) -> str:
    title = doc.metadata.get("title", "â€”")
    link = doc.metadata.get("link", "")
    pub_iso_dt = _to_utc(doc.metadata.get("published_date"))
    pub_iso = pub_iso_dt.isoformat() if pub_iso_dt else "Î†Î³Î½Ï‰ÏƒÏ„Î· Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±"
    joined = " ".join(snippets)[:1200]

    if lang == "en":
        return f"""
[SYSTEM]
Rewrite the provided sentences into a concise, neutral-tone news answer. No emojis, no exclamation tone.

RULES:
- ONE paragraph, 3â€“5 sentences.
- First sentence MUST begin: "Based on the {pub_iso} article:"
- Use ONLY the sentences below (no outside facts).
- Mention the date only in the first sentence.
- End with the source link as the LAST token: {link}
- Do NOT add disclaimers unless the sentences are irrelevant to the question.
- Do NOT produce lists or bullet points under any circumstance.

[SENTENCES]
\"\"\"
{joined}
\"\"\"

[USER QUESTION]
{user_query}
""".strip()

    return f"""
[Î£Î¥Î£Î¤Î—ÎœÎ‘]
ÎÎ±Î½Î±Î³ÏÎ¬ÏˆÎµ Ï„Î¹Ï‚ Ï€Î±ÏÎµÏ‡ÏŒÎ¼ÎµÎ½ÎµÏ‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ ÏƒÎµ ÏƒÏ…Î½Î¿Ï€Ï„Î¹ÎºÎ®, Î¿Ï…Î´Î­Ï„ÎµÏÎ¿Ï… ÏÏ†Î¿Ï…Ï‚ ÎµÎ¹Î´Î·ÏƒÎµÎ¿Î³ÏÎ±Ï†Î¹ÎºÎ® Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·. Î§Ï‰ÏÎ¯Ï‚ emojis.

ÎšÎ‘ÎÎŸÎÎ•Î£:
- ÎˆÎ½Î±Ï‚ Ï€Î±ÏÎ¬Î³ÏÎ±Ï†Î¿Ï‚, 3â€“5 Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚.
- Î— Ï€ÏÏÏ„Î· Î Î¡Î•Î Î•Î™ Î½Î± Î±ÏÏ‡Î¯Î¶ÎµÎ¹: "Î’Î¬ÏƒÎµÎ¹ Î¬ÏÎ¸ÏÎ¿Ï… Ï„Î·Ï‚ {pub_iso}:"
- Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ ÎœÎŸÎÎŸ Ï„Î¹Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ (ÎºÎ±Î¼Î¯Î± ÎµÎ¾Ï‰Ï„ÎµÏÎ¹ÎºÎ® Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±).
- Î‘Î½Î¬Ï†ÎµÏÎµ Ï„Î·Î½ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î¼ÏŒÎ½Î¿ ÏƒÏ„Î·Î½ Ï€ÏÏÏ„Î· Ï€ÏÏŒÏ„Î±ÏƒÎ·.
- ÎšÎ»ÎµÎ¯ÏƒÎµ Î¼Îµ Ï„Î¿ link Ï‰Ï‚ Î¤Î•Î›Î•Î¥Î¤Î‘Î™ÎŸ token: {link}
- ÎœÎ—Î Ï€ÏÎ¿ÏƒÎ¸Î­Ï„ÎµÎ¹Ï‚ disclaimers ÎµÎºÏ„ÏŒÏ‚ Î±Î½ Î¿Î¹ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ ÎµÎ¯Î½Î±Î¹ Î¬ÏƒÏ‡ÎµÏ„ÎµÏ‚ Î¼Îµ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ·.
- ÎœÎ—Î Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï‚ Î»Î¯ÏƒÏ„ÎµÏ‚ Î® bullets Î³Î¹Î± ÎºÎ±Î½Î­Î½Î±Î½ Î»ÏŒÎ³Î¿.

[Î Î¡ÎŸÎ¤Î‘Î£Î•Î™Î£]
\"\"\"
{joined}
\"\"\"

[Î•Î¡Î©Î¤Î—Î£Î—]
{user_query}
""".strip()


# ğŸ”¹ Î’Î±ÏƒÎ¹ÎºÎ® RAG Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î± (Î”Î™Î‘Î¤Î—Î¡Î•Î™Î¤Î‘Î™ ÎŸÎÎŸÎœÎ‘ & Î¥Î ÎŸÎ“Î¡Î‘Î¦Î—)
def generate_contextual_answer(user_query, category: str = None):
    import re
    user_query = str(user_query).strip()

    # Î•Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Ï‚ Î­Î»ÎµÎ³Ï‡Î¿Ï‚ Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î±Ï‚ ÎµÏÏÏ„Î·ÏƒÎ·Ï‚
    if not user_query or len(user_query) < 3:
        return "Î— ÎµÏÏÏ„Î·ÏƒÎ® ÏƒÎ¿Ï… ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï Î¼Î¹ÎºÏÎ® Î® Î¬Î´ÎµÎ¹Î±. Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Î½Î± ÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ ÎºÎ¬Ï„Î¹ Ï€Î¹Î¿ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿."
    if re.fullmatch(r'[\W_]+', user_query):
        return "Î— ÎµÏÏÏ„Î·ÏƒÎ® ÏƒÎ¿Ï… Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Î¼ÏŒÎ½Î¿ ÏƒÏÎ¼Î²Î¿Î»Î±. Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Î¾Î±Î½Î¬ Î¼Îµ Î»Î­Î¾ÎµÎ¹Ï‚."
    if user_query.lower() in {"hello", "hi", "hey", "Î³ÎµÎ¹Î±", "ÎºÎ±Î»Î·ÏƒÏ€Î­ÏÎ±"}:
        return "Î“ÎµÎ¹Î± ÏƒÎ¿Ï…! ÎœÏ€Î¿ÏÎµÎ¯Ï‚ Î½Î± Î¼Îµ ÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ ÎºÎ¬Ï„Î¹ Î³Î¹Î± Ï„Î¹Ï‚ ÎµÎ¹Î´Î®ÏƒÎµÎ¹Ï‚, ÏŒÏ€Ï‰Ï‚ \"Î¤Î¹ Î½Î­Î± Î³Î¹Î± Ï„Î·Î½ Ï„ÎµÏ‡Î½Î·Ï„Î® Î½Î¿Î·Î¼Î¿ÏƒÏÎ½Î·;\""

    try:
        if vectorstore is None:
            return "Î£Ï†Î¬Î»Î¼Î±: Î¤Î¿ vectorstore Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î¿."

        # 1) Similarity search ÎœÎ• score (Ï€Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ tuples) â€” Î±Ï…Î¾Î·Î¼Î­Î½Î¿ k Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ recall
        t0 = perf_counter()
        hits_with_raw = similarity_search(user_query, k=40)
        t_faiss = (perf_counter() - t0) * 1000.0

        if not hits_with_raw:
            return _fallback_no_context(user_query)

        # 1.5) Keyword boost (Î®Ï€Î¹Î¿) Î²Î¬ÏƒÎµÎ¹ Ï„Î¿Ï… query
        query_kws = _keywords_for_query(user_query)
        if query_kws:
            boosted = []
            for doc, raw in hits_with_raw:
                text = f"{doc.metadata.get('title','')} {doc.page_content or ''}".lower()
                overlap = sum(1 for k in query_kws if k in text)
                new_raw = max(0.0, float(raw) - 0.03 * overlap)  # 0.03 Î±Î½Î¬ keyword
                boosted.append((doc, new_raw))
            hits_with_raw = boosted

        # 2) Category boost (Î®Ï€Î¹Î¿) Î²Î¬ÏƒÎµÎ¹ category param
        if category:
            cat = category.lower().strip()
            boosted = []
            for doc, raw in hits_with_raw:
                doc_cat = (doc.metadata.get("category") or "").lower()
                bonus = 0.05 if doc_cat == cat else 0.0
                new_raw = max(0.0, float(raw) - bonus)
                boosted.append((doc, new_raw))
            hits_with_raw = boosted

        # 2.5) BM25 re-rank (Î¼ÏŒÎ½Î¿ Î±Î½ ÎµÎ¯Î½Î±Î¹ ÎµÎ½ÎµÏÎ³ÏŒ Ï„Î¿ flag)
        if RERANK_BM25_ENABLED:
            t1 = perf_counter()
            hits_with_raw = _bm25_rerank(hits_with_raw, user_query, top_m=10)
            t_bm25 = (perf_counter() - t1) * 1000.0
        else:
            t_bm25 = 0.0

        # 3) Î•Ï€Î¹Î»Î¿Î³Î® Î•ÎÎŸÎ£ Î¬ÏÎ¸ÏÎ¿Ï… Î¼Îµ thresholds + recency + topic flag
        chosen, topic_match = _select_single_article(hits_with_raw, query_kws=query_kws)
        if not chosen:
            return _fallback_no_context(user_query)

        # 4) Prompt Î³Î¹Î± Î•ÎÎ‘ Î¬ÏÎ¸ÏÎ¿ â€” Î Î¡Î©Î¤Î‘ Ï€ÏÎ¿ÏƒÏ€Î±Î¸Î¿ÏÎ¼Îµ extractive snippets
        lang = _detect_lang(user_query)
        snippets = _extractive_snippets(chosen, user_query, k=SNIPPETS_K, min_sim=SNIPPETS_MIN_SIM) if EXTRACTIVE_ENABLED else []
        if snippets:
            prompt = _build_prompt_from_snippets(user_query, chosen, snippets, lang)
        else:
            prompt = _build_prompt_one_article(user_query, chosen, lang, topic_match)

        # 5) Î“ÎµÎ½Î½Î®Ï„ÏÎ¹Î± LLM
        t2 = perf_counter()
        answer = generate_answer(get_llm(), prompt)
        t_llm = (perf_counter() - t2) * 1000.0

        # Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î± Ï„ÏÏ€Î¿Ï…
        if not isinstance(answer, str):
            answer = str(answer or "")
        answer = answer.strip()

        # fallback Î±Î½ Î¬Î´ÎµÎ¹Î¿/Ï€Î¿Î»Ï Î¼Î¹ÎºÏÏŒ
        if not answer or len(answer) < 10:
            return _fallback_no_context(user_query)

        # 6) Post-processing: Î±Ï†Î±Î¯ÏÎµÏƒÎ· Ï„Ï…Ï‡ÏŒÎ½ â€œTopic match:â€ / labels
        answer = _re.sub(r'(?im)^\s*(topic\s*match\s*:.*)$', '', answer).strip()
        answer = _re.sub(
            r'(?im)^\s*(title|date\s*\(utc\)|link|source|excerpt|Î¬ÏÎ¸ÏÎ¿|Ï„Î¯Ï„Î»Î¿Ï‚|Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±|ÏƒÏÎ½Î´ÎµÏƒÎ¼Î¿Ï‚|Î±Ï€ÏŒÏƒÏ€Î±ÏƒÎ¼Î±)\s*:\s*.*$',
            '',
            answer
        ).strip()
        # ÏƒÏ…Î¼Ï€Ï„ÏÎ¾ÎµÎ¹Ï‚ Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ ÎºÎµÎ½ÏÎ½ Î³ÏÎ±Î¼Î¼ÏÎ½
        answer = _re.sub(r'\n{2,}', '\n', answer).strip()

        logging.info(f"[timings] faiss_ms={t_faiss:.1f} bm25_ms={t_bm25:.1f} llm_ms={t_llm:.1f}")
        return answer

    except Exception as e:
        logging.exception(f"Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï„Î·Ï‚ ÎµÏÏÏ„Î·ÏƒÎ·Ï‚: {str(e)}")
        return f"Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï„Î·Ï‚ ÎµÏÏÏ„Î·ÏƒÎ·Ï‚: {str(e)}"


def _detect_lang(q: str) -> str:
    # Î±Ï€Î»Î® ÎµÏ…ÏÎµÏ„Î¹ÎºÎ®: Î±Î½ Î­Ï‡ÎµÎ¹ ÎµÎ»Î»Î·Î½Î¹ÎºÎ¿ÏÏ‚ Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎµÏ‚ -> el, Î±Î»Î»Î¹ÏÏ‚ en
    return "el" if re.search(r"[Î‘-Î©Î±-Ï‰]", q or "") else "en"

def _fallback_no_context(q: str) -> str:
    return ("No sufficiently recent/relevant article (within 30 days). "
            "Try a more specific query or another topic.") if _detect_lang(q) == "en" else (
            "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Î±ÏÎºÎµÏ„Î¬ Ï€ÏÏŒÏƒÏ†Î±Ï„Î¿/ÏƒÏ‡ÎµÏ„Î¹ÎºÏŒ Î¬ÏÎ¸ÏÎ¿ (ÎµÎ½Ï„ÏŒÏ‚ 30 Î·Î¼ÎµÏÏÎ½). "
            "Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Ï€Î¹Î¿ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î· Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î® Î¬Î»Î»Î¿ Î¸Î­Î¼Î±.")
